---
title: "On the normality correction of ScottKnottESD"
output: html_document
---

# Introduction

In the article "An Empirical Comparison of Model Validation Techniques for Defect Prediction Models" [1], Tantithamthavorn et al.  propose Scott-Knott Effect Size Difference (ScottKnottESD) as an extension of the Scott-Knott test. The Scott-Knott test uses the results of an ANOVA test for difference of means and uses cluster analysis to create significantly different and non-overlapping clusters of results. 

As part of the extension to Scott-Knott, the authors propose a normality correction. This page demonstrates the impact of the normality correction and is indendent as supplementary material to a response on the work by Tantithamthavorn et al. currently under review. 

# Required libraries

Before we start with the analysis, we load (and potentially install) the required R libraries. Moreover, we set a fixed seed for full reproducibility. If the seed is changed, the trends in the results remain unchanged, but the exact number may vary upon knitting this R markdown page. 
```{r, warning=FALSE, message=FALSE}
if (!require("car")) install.packages("car")
if (!require("lsr")) install.packages("lsr")
if (!require("reshape2")) install.packages("reshape2")
if (!require("effsize")) install.packages("effsize")
if (!require("ScottKnott")) install.packages("ScottKnott")

library(car)
library(lsr)
library(reshape2)
library(effsize)
library(ScottKnott)
set.seed(42)
```

# The approach for normality correction

Tantithamthavorn et al. propose to transform the performance metrics of results prior to analysis with Scott-Knott. This transformation is based on the logarithm and defined as x'=log(x+1). In the following, we will refer to this simply as log-transformation. The plot below shows the curve of the original values vs the transformed values.
```{r, echo=FALSE}
x=(0:1000)/1000
plot(x,log(x+1), type="l",col=1,main="Relationship of data with and without log transformation")
```

Below, you find a (shortened and commented) version of the implementation of ScottKnottESD implementation. The original implementation is available on GitHub [2].
```{r}
sk_esd <- function(x) {
  # log transformation  
  transformLog  <- function(y){ y <- log1p(y)}
  x <- data.frame(apply(x, 2, transformLog))
  # apply ANOVA and Scott-Knott test
  av <- stats::aov(value ~ variable, data=reshape2::melt(x)) 
  sk <- ScottKnott::SK(av, which='variable',  dispersion='s', sig.level=0.05) 
  sk$original <- sk$groups; names(sk$original) <- rownames(sk$m.inf)
  ranking <- sk$groups; names(ranking) <- rownames(sk$m.inf)
  keys <- names(ranking)
  # join groups with negligible effect sizes
  for(k in seq(2,length(keys)) ){
    eff <- unlist(effsize::cohen.d(x[,keys[k]], x[,keys[k-1]])[c("magnitude","estimate")])
    # magnitude==1 is negligible
    if(eff["magnitude"] == 1 && ranking[k] != ranking[k-1]) { 
      ranking[seq(k,length(keys))] = ranking[seq(k,length(keys))] - 1;
    }
  }
  sk$groups <- ranking
  class(sk) <- c(class(sk),"sk_esd")
  return(sk)
}
```


# Aim of these examples

This page gives examples for why using a log-transformation of results often does not solve problems, may cause new problems, and actually changes, to some degree, the results. Please note that all of the below evaluations will completely ignore the actual performance achieved. We only focus on how the log-transformation impacts the Scott-Knott test and the effect size correction based on Cohen's d, i.e., the ESD part of ScottKnottESD. 

# Data

For this small experiment, we use experimental data collected by applying cross-project defect prediction to data collected by Jurezcko and Madeyski. We used 62 software products for the predictions. For training, all products that were not from the same project as the target product were used. 

As classifiers, we used Naive Bayes and C4.5 Decision Trees. As performance metrics, we evaluated the AUC and the F-measure. 

The following code sets up the data. 
```{r}
NaiveBayesAUC = c(0.7554762,0.5330616,0.7752035,0.7835739,0.7831117,0.6742709,0.9583333,0.8029731,0.5514220,0.6793530,0.6179352,0.8000000,0.7864407,0.6000000,0.7094907,0.7368056,0.7979167,0.7813187,0.7473016,0.7873092,0.8237983,0.5898696,0.5992063,0.8019802,0.7956081,0.6087963,0.7083157,0.6338323,0.7102046,0.9117647,0.8416667,0.7585366,0.8555556,0.7457521,0.6105962,0.7249676,0.8010654,0.6826995,0.8271605,0.7253086,0.7395833,0.7836879,0.6990226,0.7408003,0.7480159,0.6623377,0.7957560,0.8064336,0.3267389,0.6693075,0.7007132,0.5736842,0.8750000,0.7265905,0.5720408,0.5702850,0.7628062,0.4628803,0.7790044,0.7326292,0.4728801,0.8437500)
DecisionTreeAUC = c(0.6969048,0.5296196,0.5391523,0.6555943,0.5948821,0.6339238,0.5439815,0.6336715,0.5281734,0.5926908,0.5493853,0.5200000,0.4627119,0.5291667,0.5671296,0.5420833,0.7300881,0.6493284,0.5717749,0.6880263,0.6314655,0.6138726,0.7063492,0.5749854,0.6848724,0.5912698,0.5587489,0.5685005,0.5497465,0.7352941,0.7000000,0.7402439,0.6944444,0.5496454,0.5025368,0.6786997,0.6336840,0.4648272,0.3410494,0.8240741,0.6406250,0.6170213,0.3965535,0.5908003,0.4900794,0.6428571,0.7400531,0.6296041,0.5076357,0.4866491,0.5923332,0.7065789,0.7361111,0.6502521,0.5653728,0.5910279,0.4766653,0.4307035,0.6305669,0.7823662,0.3964859,0.6586538)

NaiveBayesFMEAS = c(0.33333333,0.18461538,0.37333333,0.40816327,0.43609023,0.26666667,0.85714286,0.32432432,0.19305019,0.19819820,0.19217082,0.00000000,0.00000000,0.00000000,0.27397260,0.29268293,0.39130435,0.47887324,0.48920863,0.58823529,0.45390071,0.08064516,0.20000000,0.29268293,0.23255814,0.11881188,0.19230769,0.18404908,0.28688525,0.00000000,0.26086957,0.42857143,0.12500000,0.24242424,0.24324324,0.24324324,0.27380952,0.27272727,0.36363636,0.15384615,0.40000000,0.33333333,0.36956522,0.43076923,0.46153846,0.00000000,0.14285714,0.37209302,0.13173653,0.16352201,0.25000000,0.18181818,0.15384615,0.37617555,0.40856672,0.50000000,0.42040457,0.24793388,0.33613445,0.24701195,0.31775701,0.37500000)
DecisionTreeFMEAS = c(0.42622951,0.33663366,0.19298246,0.47058824,0.40318302,0.27692308,0.55172414,0.13888889,0.27355623,0.30128205,0.24864865,0.33333333,0.15384615,0.28571429,0.49438202,0.07272727,0.40000000,0.55102041,0.42105263,0.52631579,0.33830846,0.05970149,0.60000000,0.42105263,0.53333333,0.35344828,0.40000000,0.36082474,0.39857651,0.50000000,0.26086957,0.55555556,0.54545455,0.60079051,0.18279570,0.69358670,0.40703518,0.29268293,0.30769231,0.47619048,0.35294118,0.30303030,0.27027027,0.49367089,0.27272727,0.52631579,0.60000000,0.27540984,0.30434783,0.29378531,0.39024390,0.46153846,0.55555556,0.32558140,0.37651123,0.41233766,0.36545455,0.21656051,0.36000000,0.37200737,0.34920635,0.22222222)
```

The boxplots below visualize the results with and without the log transformation (NB=Naive Bayes, DT=Decision Tree, FM=F-measure). 
```{r, echo=FALSE,  }
boxplot(NaiveBayesAUC,log(NaiveBayesAUC+1),DecisionTreeAUC,log(DecisionTreeAUC+1), NaiveBayesFMEAS,log(NaiveBayesFMEAS+1), DecisionTreeFMEAS, log(DecisionTreeFMEAS+1), names=c("AUC NB","log(AUC+1) NB","AUC DT","log(AUC+1) DT", "FM NB","log(FM+1) NB", "FM DT", "log(FM+1) DT"),las=3, main="Data used to study the impact of the log-transformation", cex.axis=0.7)
```

# Impact on the normality assumption of ANOVA
Now, we test for the normaliy assumption of ANOVA with the Shapiro-Wilk test. The null hyothesis of the test is, that the data is normally distributed. 
```{r}
shapiro.test(NaiveBayesAUC)$p.value
shapiro.test(DecisionTreeAUC)$p.value
shapiro.test(NaiveBayesFMEAS)$p.value
shapiro.test(DecisionTreeFMEAS)$p.value
```
The results with the decision tree are both normally distributed (p-value > 0.05), for Naive Bayes both results are not normally distributed (p-value < 0.05). 

When we apply the proposed log-transformation, the Shapiro-Wilk test yields the following:
```{r}
shapiro.test(log(NaiveBayesAUC+1))$p.value
shapiro.test(log(DecisionTreeAUC+1))$p.value
shapiro.test(log(NaiveBayesFMEAS+1))$p.value
shapiro.test(log(DecisionTreeFMEAS+1))$p.value
```
The log-transformation does not solve the problems with the normality assumption. While the result of Naive Bayes with F-measure is now normally distributed, the results with Naive Bayes and AUC are still not normally distributed. The p-value was actually reduced, meaning the results are even more significantly non-normal. Similarly, the p-values for the results with the decision tree were also reduced, indicating that while the data is still normally distributed, it was better before the log-transformation. 

# Impact on the homoscedasticity of ANOVA
The variances of the results are the following:
```{r}
var(NaiveBayesAUC)
var(DecisionTreeAUC)
var(NaiveBayesFMEAS)
var(DecisionTreeFMEAS)
```
With the log-transformation, the variances change as follows:
```{r}
var(log(NaiveBayesAUC+1))
var(log(DecisionTreeAUC+1))
var(log(NaiveBayesFMEAS+1))
var(log(DecisionTreeFMEAS+1))
```


Now, we use Levene's test to test the homoscedasticity assumption of ANOVA. The null hypothesis of Levene's test is that the variances are the same. Levene's test is non-parametric, i.e., does not required the data to be normally distributed. Other tests, e.g., the F-test or Bartelett's test require normally distributed data and can not be applied here (see above). 
```{r}
# First transform data into data frame
resultsAUC = rbind(data.frame(result="NaiveBayesAUC",value=NaiveBayesAUC),data.frame(result="DecisionTreeAUC",value=DecisionTreeAUC))
resultsFMEAS = rbind(data.frame(result="NaiveBayesFMEAS",value=NaiveBayesFMEAS),data.frame(result="DecisionTreeFMEAS",value=DecisionTreeFMEAS))

leveneTest(value~factor(result), data=resultsAUC)$`Pr(>F)`[1]
leveneTest(value~factor(result), data=resultsFMEAS)$`Pr(>F)`[1]
```

After the log-transformation, we get the following results. 
```{r}
# First transform data into data frame
resultsLogAUC = rbind(data.frame(result="NaiveBayesAUC",value=log(NaiveBayesAUC+1)),data.frame(result="DecisionTreeAUC",value=log(DecisionTreeAUC+1)))
resultsLogFMEAS = rbind(data.frame(result="NaiveBayesFMEAS",value=log(NaiveBayesFMEAS+1)),data.frame(result="DecisionTreeFMEAS",value=log(DecisionTreeFMEAS+1)))

leveneTest(value~factor(result), data=resultsLogAUC)$`Pr(>F)`[1]
leveneTest(value~factor(result), data=resultsLogFMEAS)$`Pr(>F)`[1]
```
In both cases, the p-values change drastically. For AUC, the p-value is increased, for F-measure it is decreased. Fortunately, in both cases Levene's test yields the same results. 

# Impact on Cohen's d
Cohen's d is a measure for the significance of attributes. 

```{r}
cohensD(x=NaiveBayesAUC, y=DecisionTreeAUC)
cohensD(x=NaiveBayesFMEAS, y=DecisionTreeFMEAS)
```

After the log-transformation, we get the following results. 
```{r}
cohensD(x=log(NaiveBayesAUC+1), y=log(DecisionTreeAUC+1))
cohensD(x=log(NaiveBayesFMEAS+1), y=log(DecisionTreeFMEAS+1))
```
The values change slightly. Again, in one case slightly weaking the effect size, in the other strengthening it.  

# Why do these changes matter?

The changes due to the log-transformation matter, because their impact cannot be predicted. While they can help to make data normally distributed, this is not guaranteed. They may actually have the opposite effect and transform normally distributed data into not normally distributed data, as the example below shows. 
```{r}
count.both.normal = 0
count.both.notnormal = 0
count.onlylog.normal = 0
count.onlylog.notnormal = 0
for( i in 1:100 ) {
  data = rnorm(100,mean = 0.5,sd = 0.3)
  testres.nolog = shapiro.test(data)
  testres.log = shapiro.test(log(data+1))
  if( testres.nolog$p.value>=0.05 && testres.log$p.value >= 0.05) {
    count.both.normal = count.both.normal+1
  }
  else if( testres.nolog$p.value<0.05 && testres.log$p.value<0.05) {
    count.both.notnormal = count.both.notnormal+1
  }
  else if( testres.nolog$p.value>=0.05 && testres.log$p.value<0.05) {
    count.onlylog.notnormal= count.onlylog.notnormal+1
  }
  else if( testres.nolog$p.value<0.05 && testres.log$p.value>=0.05) {
    count.onlylog.normal = count.onlylog.normal+1
  }
}
print(paste("Both normal:",count.both.normal))
print(paste("Both not normal:",count.both.notnormal))
print(paste("Only normal after log transformation:",count.onlylog.normal))
print(paste("Only normal without log transformation:",count.onlylog.notnormal))
```

And while they may not effect the homoscedasticity, our experiment above shows, that the p-value may change a lot. The example below exemplifies how this could have a negative effect. 
```{r}
xlarge= rep(c(0.95,0.97,0.94,0.96,0.84,0.86,0.86,0.95),40)
xsmaller = xlarge-0.3
xdataframe = rbind(data.frame(result="xlarge", value=xlarge), data.frame(result="xsmaller", value=xsmaller))
logxdataframe = rbind(data.frame(result="xlarge", value=log(xlarge+1)), data.frame(result="xsmaller", value=log(xsmaller+1)))
leveneTest(value~factor(result), data=xdataframe)$`Pr(>F)`[1]
leveneTest(value~factor(result), data=logxdataframe)$`Pr(>F)`[1]
```
Without transformation, xlarge and xsmaller have the exact same variance and the data is homoscedastic. However, when the logarithm is applied, the relative distances between the data points change, affecting the variances to such a degree that homoscedasticity is not fulfilled anymore. While this example is artifical, a difference in 0.3 between performance values of different models is actually something that could happen easily in experiments. 

Another aspect which we have not explored above is, that log-transformation might actually change the results of Scott-Knott itself. In the example below, we draw three random vectors, that fulfill the requirements of ScottKnott, i.e., are normally distributed, homoscedastic, and independent.
```{r}
count.same = 0
count.different = 0
for( i in 1:100 ) {
  x1 = rnorm(100,mean = 0.5, sd = 0.1)
  x2 = rnorm(100,mean = 0.6, sd = 0.1)
  x3 = rnorm(100,mean = 0.625, sd = 0.1)

  x123dataframe = rbind(data.frame(result="x1", value=x1), data.frame(result="x2", value=x2), data.frame(result="x3", value=x3))
  sk123 = SK(aov( value ~ factor(result), x123dataframe))

  logx123dataframe = rbind(data.frame(result="x1", value=log(x1+1)), data.frame(result="x2", value=log(x2+1)), data.frame(result="x3", value=log(x3+1)))
  logsk123 = SK(aov( value ~ factor(result), logx123dataframe))

  if( all(sk123$groups==logsk123$groups) ) {
    count.same = count.same+1
  } else {
    count.different = count.different+1
  }
}
print(count.same)
print(count.different)
```
Sometimes the results with using the log-transformation are different. The reason for these changes is the same as for the changes for the homoscedasticity: the relative distances between the data points change. Since the cluster analysis of Scott-Knott is based on the within group sum of squares, the cluster analysis is also affected by the changes. Our results above show that this may lead to different groupings, in case the log-transformation is used.

As our results above already show, the impact on Cohen's d of the log transformation is relatively small.  However, it may be just enough to flip a result over the 0.2 threshold for negligible effect size. Consider the following example:
```{r}
x1= rep(c(0.95,0.97,0.94,0.96,0.84,0.86,0.86,0.95),40)
x2 = x1-0.01
cohensD(x=x1, y=x2)
cohensD(x=log(x1+1), y=log(x2+1))
```
Here, the log transformation did just that. With the log transformation, the effect size is negligible, without it, it is not. For the ScottKnottESD, this means with the log-transformation, the clusters would be merged, without it, they would not.

Here is the result of standard Scott-Knott:
```{r}
x2dataframe = rbind(data.frame(result="x1", value=x1), data.frame(result="x2", value=x2))
summary(SK(aov( value ~ factor(result), x2dataframe)))
```

And here of ScottKnottESD:
```{r, warning=FALSE, message=FALSE}
# ScottKnottESD
sk_esd(data.frame(x1,x2))$groups
```

Again, the result is changed due to the log-transformation. 

(Please note that in the example above, neither x1, nor x2 are normally distributed. Thus, Scott-Knott should actually not be applied at all. However, since the authors state in the paper that ScottKnottESD makes "no assumptions about the underlying distribution" [1], we used the example anyhow.)

# Conclusion
Our results above show, that the impact of the log-transformation cannot be predicted and the transformation does not assure that the requirements of ANOVA are fulfilled, i.e., the data is normally distributed and homoscedastic. We do not consider the independence here, as this should be addressed as part of the experiment setup.

Therefore, we urge to be cautios with applying the log-transformation to performance metrics and in case the requirements of ANOVA are not met, rather switch to a non-parametric statistical test (e.g., Friedman-Nemenyi). 

# References
[1] Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E. Hassan, Kenichi Matsumoto: An Empirical Comparison of Model Validation Techniques for Defect Prediction Models, IEEE Transactions on Software Engineering, 43(1):1-18, 2017

[2] https://github.com/klainfo/ScottKnottESD